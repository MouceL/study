第二章 分布式集群

2.1 空集群

​	一个节点就是一个es 实例，集群由多个节点组成，它么有相同的clusterName, 协同工作，分享数据和负载。

​	集群中有一个节点是主节点 master , 它将临时管理集群级别的状态变更，例如创建删除索引，增加或者移除节点。
    但是它不参与文档级别的变更和搜索，这意味着随着流量的增长，该主节点不会成为集群的瓶颈。用户可以与任意节
    点通信，每个节点都知道文档存在哪个分片上，它会转发请求到相应的节点上。与用户通信的节点负责收集数据并返回给用户。



2.2 集群健康

三种健康状态    GET /_cluster/health

green 	所有主要分片和复制分片都可用

yellow   主分片可用，但不是所有复制分片都可用

red  不是所有主分片都可用

GET /_cluster/health



2.3 添加索引

​	为了存储数据，需要对文档进行索引，索引指向多个物理分片，是一个逻辑上的命名空间，平时说数据在 logmining-search-2020这个索引中，
    其实对应着后端的物理分片。分片是最小级别的工作单元，一个分片就是一个lucence实列。分片可以分为主分片和复制分片，当索引创建完成时，
    要指定主分片个数，并在以后不再改变。复制分片可以随时调整。在申请创建模板时会设置相关参数

```
PUT /logmining
{
	"settings":{
		"number_of_shareds":3,
		"number_of_replicas":1
	}
}
logming 这个索引分配了三个主分片，每个组分配有一个副本
```


如果这些只有一个单节点集群，如本地计算机，如果按照以上设置，那么三个主分片都会被分配到同一个节点，

```
{
   "cluster_name":          "elasticsearch",
   "status":                "yellow", <1>
   "timed_out":             false,
   "number_of_nodes":       1,
   "number_of_data_nodes":  1,
   "active_primary_shards": 3,
   "active_shards":         3,
   "relocating_shards":     0,
   "initializing_shards":   0,
   "unassigned_shards":     3 <2>
}
```

如果 GET /_cluster/health， 会返回以上信息，集群状态是yellow ，也就是主分片可用，但是不是所有复制分配可用，其实就没有复制分片。
可以从<2>看出，复制分片没有 被分配。



2.4  故障转移

如果集群只有一个节点，那么一旦该节点挂掉，那么数据就都丢失了，所以分布式的服务都采用备份的方式减少丢失数据的风险。

现在启动第二个节点，跟第一个节点拥有相同的clusterName，那么它就会加入同一个集群。

![双节点集群](https://raw.githubusercontent.com/looly/elasticsearch-definitive-guide-cn/master/images/elas_0203.png)

加入后，主分片就自动分配到两个节点上，如果不是两个节点同时挂掉就不会出现丢失数据。

```Javascript
{
   "cluster_name":          "elasticsearch",
   "status":                "green", <1>
   "timed_out":             false,
   "number_of_nodes":       2,
   "number_of_data_nodes":  2,
   "active_primary_shards": 3,
   "active_shards":         6,
   "relocating_shards":     0,
   "initializing_shards":   0,
   "unassigned_shards":     0
}
```

​	这个跟kafka 集群有着类似的架构方式，一个topic 可以拥有多个partition, 每个partition 都有复制分片，用上图来对应的化，
    就是logmining 这个topic 拥有三个partition， 每个partition 拥有一个副本。


2.5 横向扩展

​	随着应用需求的增长，我们启用第三个节点，那么集群会自己重新组织自己。以上面例子来看，每个节点上都会存在两个分片，这相比与
    上图，每个节点的分片少了一个，也就意味着每个节点上的分片将获得更多的硬件资源。分片本身就是一个完整的 lucence 搜索引擎，
    它可以使用单一节点的所以资源，这个例子中，有6个分片，理论上可以扩展到6个节点，3个节点放主分片，3个节点放复制分片，只是
    这样做的成本提升了。


2.6  继续扩展

​	如果我们要扩展到6个以上的节点该怎么做？ 

​	主分片的数量在创建的时候就已经确定，那么可以通过增加副本数目来扩展，因为就算是复制分片也是一个独立的搜索引擎，它也可以处
    理读请求，所以增加副本数不仅降低了数据的丢失风险，也提高了搜索的吞吐量。

复制分片的数量可以用setting 设置

```
PUT /logmining/_settings
{
   "number_of_replicas" : 2
}
```


![三节点两复制集群](https://raw.githubusercontent.com/looly/elasticsearch-definitive-guide-cn/master/images/elas_0205.png)



当然一味的增加复制节点的个数也是不行的，因为一个节点的性能是一定的，分在该节点的分片数量增加了，那么每个分片分到的资源就减少了。



2.7 应对故障

如果kill 一个节点，如果杀掉的是主节点，那么会重新选取一个主节点，

![杀掉一个节点后的集群](https://raw.githubusercontent.com/looly/elasticsearch-definitive-guide-cn/master/images/elas_0206.png)

假设kill 的就是主节点，那么会选举一个新的主节点 node2

    主分片1 和2 在kill node1 时已经丢失所以es 会升级复制分片成为新的主分片，这时状态会变成yellow ，
    因为并不是所有的复制节点都是可用的，如果我重启node1 ，那么它会恢复green 状态，在node1重启后，
    可能会存在数据与当前主分片不一致的情况，只要修改变更的那一部分，集群就完成了恢复。

